{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hypernetworks_in_keras_and_tf2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hummosa/Hypernetworks_Keras_TF2/blob/master/Hypernetworks_in_keras_and_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA6CfJ159Qke",
        "colab_type": "text"
      },
      "source": [
        "#Guide to Hypernetworks in Keras and Tensorflow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFscRdMm4-R_",
        "colab_type": "text"
      },
      "source": [
        "This is a keras implementation of hypernetworks, which are typically a pair of networks where one generates the parameters (weights) of the other [(Ha, et al., 2016)](https://arxiv.org/abs/1609.09106). Keras layer implementation exposes the parameters of a layer as two modifiable properties: ‘kernel’ and ‘bias’, which allows assiging them new values during inference. \n",
        "\n",
        "This code will separate the hypernetwork into two keras models: an inference model which will perform the inference task  (for e.g. classify handwritten digits), and a hyper model, which will generate the parameters of the inference model for each input example. We will demonstrate using a convolutional network as the inference model to classify MNIST digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHUHlg3PpsFq",
        "colab_type": "code",
        "outputId": "9b653bc6-1261-465f-9dcd-bdaa064c519a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/74/c6d43d2a9a26ea430acd804068bbaf2fdf14dd470f543d113588b1f80828/tf_nightly_gpu_2.0_preview-2.0.0.dev20190611-cp36-cp36m-manylinux1_x86_64.whl (343.8MB)\n",
            "\u001b[K     |████████████████████████████████| 343.8MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/d5/3ef112818fe4181e9bf1a1233a0cd8f9ffb63f386f268701aa205bf04384/tensorflow_estimator_2.0_preview-1.14.0.dev2019061100-py2.py3-none-any.whl (436kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 41.4MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.8)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/78/8a09284bbc5bb43332dabddd15fe098363a9f0d27e7468e31fb183394bc9/tb_nightly-1.14.0a20190611-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, google-pasta, tb-nightly, tf-nightly-gpu-2.0-preview\n",
            "Successfully installed google-pasta-0.1.7 tb-nightly-1.14.0a20190611 tensorflow-estimator-2.0-preview-1.14.0.dev2019061100 tf-nightly-gpu-2.0-preview-2.0.0.dev20190611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnNVpdnU94fi",
        "colab_type": "code",
        "outputId": "e9b7d758-b3d4-4024-8360-1c1daab4973d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import tensorflow and check version\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('tensorflow version: {}'.format(tf.__version__))\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 2.0.0-dev20190611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAd3QYjg-AjH",
        "colab_type": "text"
      },
      "source": [
        "For this tutorial will use the MNIST dataset to demonstrate the setup. The following code will download and prepare the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVQYaADmbd7Y",
        "colab_type": "code",
        "outputId": "2b6fbe04-998d-46bf-b8e3-85b21ebc28d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# convert to float32 and normalize. \n",
        "x_train = x_train.astype('float32') /255\n",
        "x_test = x_test.astype('float32')   /255\n",
        "\n",
        "# one-hot encode the labels \n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "# add a channel dimension to the images\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28,1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28,1)\n",
        "\n",
        "\n",
        "# Define image dimensions\n",
        "img_h = 28\n",
        "img_w = 28\n",
        "img_c = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIPU7Szr-tU_",
        "colab_type": "text"
      },
      "source": [
        "### Inference model:\n",
        "We now build the inference model, a simple convolutional network, with a fully connected layer on top, that we will use to classify MNIST digits. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkMNzey0-uQb",
        "colab_type": "code",
        "outputId": "aa802f85-1fd0-4d28-c670-a518149a2095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "infer_model = tf.keras.models.Sequential(name='infer_model')\n",
        "infer_model.add(tf.keras.layers.Input(shape=(img_h, img_w, img_c), name='input_x' ))\n",
        "infer_model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu') )\n",
        "infer_model.add(tf.keras.layers.MaxPool2D() )\n",
        "infer_model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu') )\n",
        "infer_model.add(tf.keras.layers.MaxPool2D() ) \n",
        "infer_model.add(tf.keras.layers.Flatten() )\n",
        "\n",
        "infer_model.add(tf.keras.layers.Dense(10, activation= 'softmax', name='out_layer') )\n",
        "\n",
        "infer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"infer_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 10)                8010      \n",
            "=================================================================\n",
            "Total params: 17,578\n",
            "Trainable params: 17,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2irt-_gV-r0z",
        "colab_type": "text"
      },
      "source": [
        "Note that this model has a total of 17,578 parameters that need to be generated by the hyper model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uFLUlgxEFbq",
        "colab_type": "text"
      },
      "source": [
        "### Hyper model:\n",
        "Let us now define the hyper model with 2 convolutional layers and a fully connected layer on top to produce a latent embedding of size 784. The embedding is then fed into a stack of 3 transpose convolutional layers that produce a large number of values, which will be used as parameters for the inference model.\n",
        "\n",
        "Note that the last layer uses a tanh activation function which produces values between -1 and 1. This allows generation of parameters with negative values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obmi-YOfEUhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyper_model_x = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer(input_shape=(img_h, img_w, img_c)),\n",
        "        tf.keras.layers.Conv2D(16, (3,3), activation='relu') ,\n",
        "        tf.keras.layers.MaxPool2D() ,\n",
        "        tf.keras.layers.Conv2D(8, (3,3), activation='relu') ,\n",
        "        tf.keras.layers.MaxPool2D() ,\n",
        "        tf.keras.layers.Flatten() ,\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(units=784, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            filters=16, \n",
        "            kernel_size=3,\n",
        "            strides=(2, 2),\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu),\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            filters=8,  \n",
        "            kernel_size=3,\n",
        "            strides=(2, 2),\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu),\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            filters=2, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='tanh'),\n",
        "        tf.keras.layers.Flatten()\n",
        "    ], name='hyper_model'\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVoEllG5FSzY",
        "colab_type": "text"
      },
      "source": [
        "To apply parameters to the inference model, we define a function 'parametrize_model', which consumes a tensor of generated parameters to parametrize the weights and the biases of each layer in a model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4OJzYqJR-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameterize_model(model, weights):\n",
        "    # function to parametrizes all the trainable variables of model using the stream of weight values in weights\n",
        "    # This assumes weights are passed a single batch.\n",
        "    weights = tf.reshape( weights, [-1] ) # reshape the parameters to a vector\n",
        "    \n",
        "    last_used = 0\n",
        "    for i in range(len(model.layers)):\n",
        "        # check to make sure only conv and fully connected layers are assigned weights.\n",
        "        if 'conv' in model.layers[i].name or 'out' in model.layers[i].name or 'dense' in model.layers[i].name: \n",
        "            weights_shape = model.layers[i].kernel.shape\n",
        "            no_of_weights = tf.reduce_prod(weights_shape)\n",
        "            new_weights = tf.reshape(weights[last_used:last_used+no_of_weights], weights_shape) \n",
        "            model.layers[i].kernel = new_weights\n",
        "            last_used += no_of_weights\n",
        "            \n",
        "            if model.layers[i].use_bias:\n",
        "              weights_shape = model.layers[i].bias.shape\n",
        "              no_of_weights = tf.reduce_prod(weights_shape)\n",
        "              new_weights = tf.reshape(weights[last_used:last_used+no_of_weights], weights_shape) \n",
        "              model.layers[i].bias = new_weights\n",
        "              last_used += no_of_weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7n5JgFfIInp",
        "colab_type": "text"
      },
      "source": [
        "### The training loop:\n",
        "We are now ready to define the main training loop. Eager execution is enabled by default in tensorflow 2.0, which provides more control over the training process. Note that the loss function is differentiated with respect to the hyper model parameters only. In fact, the parameters of the inference model are no longer considered trainable by keras (can check by running infer_model.summary()). This loop updates the parameters of the hyper model only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgoCcsLqNQ1",
        "colab_type": "code",
        "outputId": "915c5fef-b076-4668-90a0-6ce492da8ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Define accuracy metrics for validation\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3) \n",
        "\n",
        "loss_accum = 0.0\n",
        "batch_size = 1\n",
        "for step in range(1, 6001):\n",
        "  idx = np.random.randint(low=0, high=x_train.shape[0], size=batch_size)\n",
        "  x, y = x_train[idx], y_train[idx]\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    # Predict weights for the infer model\n",
        "    generated_parameters = hyper_model_x(x)\n",
        "    parameterize_model(infer_model, generated_parameters)    \n",
        "    \n",
        "    # Inference on the infer model\n",
        "    preds = infer_model(x)\n",
        "\n",
        "    loss = loss_fn( y, preds)\n",
        "    loss_accum += loss\n",
        "    train_acc_metric( y, tf.expand_dims(preds, 0)) # update the acc metric\n",
        "\n",
        "    if step % 1000 == 0: \n",
        "      loss_accum = 0.0\n",
        "      var = generated_parameters.numpy()\n",
        "      print('statistics of the generated parameters: '+'Mean, {:2.3f}, var {:2.3f}, min {:2.3f}, max {:2.3f}'.format(var.mean(), var.var(), var.min(), var.max()))\n",
        "      for val_step in range(500): # \n",
        "        idx = np.random.randint(low=0, high=x_test.shape[0], size=batch_size)\n",
        "        x, y = x_test[idx], y_test[idx]\n",
        "        generated_parameters = hyper_model_x(x)\n",
        "        parameterize_model(infer_model, generated_parameters)    \n",
        "        preds = infer_model(x)\n",
        "        val_acc_metric( y, tf.expand_dims(preds, 0)) # update the acc metric\n",
        "      print('\\n Step: {}, validation set accuracy: {:2.2f}     loss: {:2.2f}'.format(step, float(val_acc_metric.result()), loss_accum))\n",
        "      val_acc_metric.reset_states()\n",
        "         \n",
        "        \n",
        "    # Train only hyper model\n",
        "    grads = tape.gradient(loss, hyper_model_x.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, hyper_model_x.trainable_weights))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "statistics of the generated parameters: Mean, -0.074, var 0.033, min -0.991, max 0.995\n",
            "\n",
            " Step: 1000, validation set accuracy: 0.97     loss: 0.00\n",
            "statistics of the generated parameters: Mean, -0.087, var 0.069, min -1.000, max 0.999\n",
            "\n",
            " Step: 2000, validation set accuracy: 0.96     loss: 0.00\n",
            "statistics of the generated parameters: Mean, -0.069, var 0.028, min -0.990, max 0.980\n",
            "\n",
            " Step: 3000, validation set accuracy: 0.92     loss: 0.00\n",
            "statistics of the generated parameters: Mean, -0.074, var 0.024, min -0.996, max 0.983\n",
            "\n",
            " Step: 4000, validation set accuracy: 0.95     loss: 0.00\n",
            "statistics of the generated parameters: Mean, -0.089, var 0.062, min -1.000, max 0.999\n",
            "\n",
            " Step: 5000, validation set accuracy: 0.94     loss: 0.00\n",
            "statistics of the generated parameters: Mean, -0.082, var 0.050, min -0.999, max 0.998\n",
            "\n",
            " Step: 6000, validation set accuracy: 0.97     loss: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcnIhobpqunK",
        "colab_type": "text"
      },
      "source": [
        "Two issues arise in building a hypernetwork in keras. First is managing mini-batching to speed up training on GPUs, and the second relates to initializing the weights. \n",
        "The need for a different weight matrix for each input sample introduces significant challenges in using mini-batches during training. While it is possible to create custom keras layers that can handle storing a batch of weights and biases for each layer, we kept batch_size at 1 for the purposes of this tutorial.\n",
        "On another front, the intial values of a neural network parameters may significantly impact training dynamics. Keras, by default, uses the [Glorot initializer ](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), which factors in the number of connections coming from previous layers. In a hypernetwork setup, assigning weights directly to a network sidesteps this initilaization that keras normally handles automatically. Accordingly, it is important to moniter and the statistics of the generated parameters and consider how their mean, variance and range might affect training dynamics. For the purposes of this guide, we found it important to use a tanh activation function in the last layer of the hyper model, which provided values cenetered around 0 with a range from -1 to 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha6mGmQKivqO",
        "colab_type": "text"
      },
      "source": [
        "Finally let's look at a histogram of the generated parameters. There is a sharp peak around very small negative values  around -0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6p9Wf3TKeOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = plt.hist(generated_parameters, bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFiPFx9OjiWh",
        "colab_type": "text"
      },
      "source": [
        "####References:\n",
        "1) Ha, D., Dai, A., & Le, Q. V. (2016). Hypernetworks. arXiv preprint arXiv:1609.09106.\n",
        "    \n",
        "2) Glorot, X. & Bengio, Y.. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, in PMLR 9:249-256\n"
      ]
    }
  ]
}